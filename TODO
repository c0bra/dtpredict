TODO list for Perl module DateTime::Even::Predict

------------------------LEGEND-------------------------
[TODO] - Something I have to do
[TODO/DONE] - Something I had to do and did
[BUG] - Bug I found
[FIX] - Bugfix
[FIX/NOT] - Bugfix that didn't actually fix anything
[PROBLEM] - Problem with methodology
[IDEA] - Self-explanatory
[NOTE] - Also self-explanatory
-------------------------------------------------------

2/11/2010
* [TODO] Write a test that makes sure a prediction that is good with regard to the distinct buckets
  is removed if it is bad with regard to the interval buckets.

2/10/2010
* Regarding the problem with interval buckets and clustering from yesterday: if we thinking about
  how clustering works, what we are doing is completely changing the rules about how the dates
  relate to each other. Instead of there being 18 dates we say that there are technically 6 dates,
  each of which is a cluster containing 3 dates. The rules about how the dates relate to each other
  as a whole now ONLY applies at the cluster level, and then within the cluster there is another set
  of rules but it only applies to that cluster.
* [BUG] Sudden thought: some DateTime::Duration's may not have accessors available for dateparts they
  don't have. That is, if a date is just YMD the duration might not have a 'seconds' accessor and
  thus our interval training method will die ungracefully.
  - Nothing bad happens. It makes predictions successfully.


2/9/2010
* [PROBLEM] I just ran another test and DateTime::in_units() is definitely NOT working. Using it on the
  duration object for "1/1/2010 - 1/1/2009" results in 0, not 365.
  - [FIX/NOT] I have now replaced this with a rather ugly string of nested calls to the delta_* methods in
    DateTime::Duration. See the bottom of this page: http://datetime.perl.org/index.cgi?FAQSampleCalculations
    at the "How can I calculate the difference in days between dates" heading.
    - [TODO] Write a test to check this. Using interval_buckets => ['days'] on a two dates separated
      by 1 year should suffice. We will also wants tests for all the other delta_* accessors.
    - [TODO/DONE] Turn the ugly coderef calls into a simple method where you pass in the two dates and the
      accessor name so it's all in one place.
    - [BUG] Nothing is fixed. delta_days() is the only delta_ method that exists in DateTime. The others
      work differently and are in DateTime::Duration.
* [BUG] - Clustering being on by default is causing failures in some tests.
* [TODO] - Right now clustering is retraining the DTP instance by replacing the supplied dates with the
  cluster centroids. That might not be the best way to go about it.
* [BUG] - There is no delta_years() method in DateTime::Duration, it has to do with how date math works.
  We'll have to find those intervals some other way.
* [IDEA] - For fixing the search range problem (bug note from yesterday). Perhaps for the interval side we
  can take the mean epoch interval between takes and multiply it by the 'stdev_limit' option? I need to
  write a test for this either way.
  - [PROBLEM] - A huge problem with this is if we are creating a fake bucket with the cluster intervals
    then the difference in epoch seconds is going to be HUGE and will create a huge range.
    - This problem will exist for any bucket with a large variance in values. Like for dates that are all
      500 years apart.
* [BUG] - _trim_dates() only works on distinct_buckets. It needs to work on interval buckets too.
* [PROBLEM] - Right now we send the first prediction we find, but wouldn't it be better to find more and
  then send the best?
* [FIX] - Fixed date trimming issue. Dates should trim properly now
* [BUG] - Clustering just is not working when it comes to finding the right element in a cluster. We may
  need to use the interval between clusters as a bucket so we can restrict the predictions we make. Right
  now if we have clusters where the outermost elements are ALWAYS a certain interval apart, i.e. with a 
  standard deviation of 0, we will still make bad predictions because we're not using that interval for
  anything.
  - What if we create a fake bucket prefilled with all the statistical information. That's hacky but it
    could work. One problem to solve is we have to compare difference in dates using epoch seconds. We
    can't use the 'seconds' accessor because it only shows the difference in clock seconds, not actual
    interval seconds.
    - I've added some hacky bits to _convert_seconds() and _get_date_interval() so it works properly
      with the fake cluster bucket.
* [BUG] - I have the fake clustering bucket on now, and it appears to be working, however the good prediction
  for 13-cluster.t is failing to fall within the stdev for the 'days' bucket. The stdev is 2.17 and this
  date is 5, as it's 5 days from the closest date, which is CORRECT because the mean distance between closest
  member elements of the clusters is 5, and the stdev for that is 0. So it HAS to be 5 days away, but also
  must fit in the normal days stdev. That might not work :(


2/8/2010
* [NOTE] - I may have found a solution to the clustering problem. The S-means algorithm starts with an initial
  value for k (default 1) and then adds and removes clusters as needed. It relies on a user-defined
  threshold to determine whether a data point should be added to a cluster or put into a new cluster
  and if I use the standard deviation for the distances of each data point from the one before it then
  it seems to work perfectly in situations where data should not be clustered, i.e. given
  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] as the data points it will provide 10 clusters. If we add "12" onto
  the data list it will provide 11 clusters.
  - [NOTE] However I just tried adding "15" instead of 12 onto the list and it creates 6 clusters
    rather than 11, essentially splitting the data in half. This might not be what we want, but it
    could be as the only thing we'll be using clustering for currently is finding the initial search point
    to look for date predictions from. If it gives a proper distance to use to find that search point then
    it could be OK.
* [IDEA] We might want to add the capability to use multiple search points. For situations where we are
  clustering it's possible that the next date could go in the most recent cluster, or it could go in a
  new cluster. Searching from both the most recent date in the most recent cluster and the new cluster
  start point would be a good idea.
  - [IDEA] We could also measure the average and standard deviation in the number of elements per cluster
    to determine if it should go in the most recent cluster or a new one.
  - [NOTE] This might be moot if we cannot rely on our clustering algorith to give the "true" number oc
    clusters. With S-means we see clustering happen when we might not want it to but can probably still
    provide good results. For instances like [1..5, 15] it might not matter whether the new date goes in
    the most recent cluster or in a new one. They could be so close that the end result would be the same.
    However for an instance like [1..10, 200] it could be very useful to have multiple search points.
  - [NOTE] Searching for a date in a new cluster can probably produce a result just as feasible (in terms
    of date-part and interval validity), however we would probably want to add some metric for choosing
    between the most recent cluster and a new cluster, such as the most recent cluster's variance in the
    number of member elements from the average, and the new cluster's variance in distance from the most
    recent date (perhaps).
* [NOTE] We may want to add the bucket statistics into train(). They'll need to be there so we can
  auto-enable the buckets we want if no profile is specified.
* [PROBLEM] How do we trim dates when there is no profile set? Do we need to?
* [PROBLEM] Date intervals aren't working quite like we want them to. In 13-cluster.t it's saying there's
  an average interval of 3 days with a stdev of 0. That's SORT OF true, in that the day of the week of each
  date varies from the one before it by 3 days (the centroids are each 10 days apart). However if we want
  to use the full delta of days (i.e. 10 days, not 3) the DateTime FAQ says we need to use delta_days()
  twice. So the question is do we want to switch it so that the full delta is calculated rather than just
  that date-part's delta? Or do we maybe want BOTH calculated? The thing is, the full delta for each
  date-part is going to be equal. A full delta of 7 days is 1 week is 604,800 seconds, and it doesn't matter
  which interval bucket we enable for prediction. However a single date-part delta between two dates will
  vary from bucket to bucket. I can see how both would be good to have. I just don't know which direction to
  go in.
  - [NOTE!] using DateTime::in_units() seems to provide the full delta result.
* [BUG] The way we are creating search ranges is improper. It should not be tied to the standard deviation
  of the bucket in question. If the start date that we are supplied is outside that standard deviation then
  we won't find a good prediction. On the other hand if we are given a wide enough search range then we have
  a chance to find a good prediction, and the deviation checks in the *date_descend() methods will prevent
  use from giving bad predictions despite the search range. THE PROBLEM is deciding what range to use.
* [IDEA] Rather than using the epoch intervals between cluster centroids, should we use the interval between
  the first and last members of each cluster? For a very wide date cluster if we start searching at the new
  centroid rather than near where we suspect the first member should go, we might miss the prediction.

2/5/2010
* [NOTE] Both the "jump" method and silhouette testing have the same problem when it comes to identifying
  the proper number of clusters: neither can determine whether clustering is appropriate or not. Specifically
  the "jump" method will always show clustering with "1" as the number of clusters having the best distortion.
  And the silhouette method cannot be used to test with just 1 cluster as it has to have other centroids to
  compare against.
  - [IDEA] What if we did a reverse test starting with a number of clusters equal to the number of data
    elements and go backwards? If the fit remains good or gets better going towards the maximum then maybe
    clustering is inappropriate? OR perhaps it will ALWAYS get better going towards the maximum.
  - [IDEA] Maybe we could count back from the maximum number of clusters a number of steps equal to the
    square root of the maximum number of clusters, rounded up (so 4 for 10 clusters, 23 for 500 clusters)
    and use moving averages on the silhouette average dissimilarity to see if there is a downwards trend. Then
    use the same number of steps to check upwards from 2 clusters. If there is a number of clusters in the
    bottom section that has a greater average dissimilarity than the one with the max avg dissimilarity in the
    top section, then the clustering operation needs to be fully done. However if there is no good clustering
    in the bottom section and the avg dissimilarities trend downwards from the max number of clusters, then
    clustering is probably not a good fit and we can skip it. This is all assumptions but I haven't found a
    test case so far that disproves this, although admittedly I have only tested a handful of data sets.
    - Problem: clustering takes (possibly) exponentially longer to cluster as k increases.
* [NOTE] So far the silhouette method seems about 10% faster in my simple benchmarking, which might not
  be the most accurate metric but it's probably close enough.


2/4/2010
* [IDEA] It may be possible to dynamically discover which date-parts to use for predictions by measuring
  ALL of them (i.e. get their stdev and variance) and see which ones have the most importance statistically
  (lowest stdev, I believe) and then scale the weight of each bucket to each other. So the most important
  bucket gains a weight of 1, and each bucket scales to that one based on the size of its standard deviation.
    - One problem with this is larger date-parts (i.e. years or epoch second intervals) will have larger
      standard deviations than, say "day of week". Perhaps the stdev can be scaled to the average size of
      the items in the set. OR perhaps there's already an established way of normalizing stdevs to a scale
      of 0 to 1.
      - It looks like this is called The Coefficient of Variation: http://en.wikipedia.org/wiki/Coefficient_of_variation
      	"The coefficient of variation is useful because the standard deviation of data must always be understood in the
      	context of the mean of the data. The coefficient of variation is a dimensionless number. So when comparing
      	between data sets with different units or widely different means, one should use the coefficient of
      	variation for comparison instead of the standard deviation."


2/2/2010
* [IDEA] It would be be cool if you could pass your own buckets in with a certain type, so you could, say,
  look for recurrence based on intervals of 6 seconds, or 18 days, whatever.
* We need to be able to handle recording more than one interval per diff. If the dates are all offset from
  each other by 1 day 6 hours (May 1, 3:00; May 2, 6:00), we can't be predicting a new date that's
  exactly 1 day after the most recent one.
    ^ The best way to do this is probably to record intervals as epoch seconds, so everything is taken
      into account. Maybe record epoch seconds in addition to whole regular intervals like days & hours.


2/1/2010
* [TODO] Add prediction optionss to Predict::new(), and then allow predict() to override them or add news ones
* [TODO] add_date() and add_dates() needs to properly validate dates passed in


1/27/2010
* [BUG] Profile.pm needs to check bucket names that get passed in to see if they actually exist. Right now
  it tries to clone them and dies ungracefully.
  - [TODO] Write a test for this as well.


12/22/2009
* Can we combine the interval and distinct buckets into one full bucket list? The names should mean there's no
  collisions (just confusion, maybe too much) and the 'type' identifier says how to use it. Probably not a
  good idea.


12/18/2009
* Is there a way to use import() outside of a BEGIN block so that export tags can be imported?
* Change new() and predict() so options can be globally set in the object and overridden in
  each call to predict().
* We could create a bucket for is_weekend_day and create a custom callback for the accessor.
  We'd have to do duration differently, though.


12/17/2009
* Finish writing up pod so module can be alpha-released on CPAN
* Right now we are trimming off any date-part that is smaller than the smallest bucket we
  have turned on. We need to make it so this is done in the comparisons, rather than
  actually modifying dates we are given. OR MAYBE NOT, because of truncate()?


12/16/2009
* Due to possible DoS attack that can be done through very large duration operations (thousands
  of years in the future, etc), we'll need to add some sort of protection, I think.


??/??/????
* Add a clustering() method so that clustering can be turned on or off whenever
